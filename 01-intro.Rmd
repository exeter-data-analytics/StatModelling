# Introduction

```{r, echo=FALSE}
IMG <- "_img" # image path
```

## Motivation

Scientists acquire knolwedge about the state of nature (the real-world) by collecting data.
For example:

```{r, echo=FALSE, out.width="600px"}
knitr::include_graphics(file.path(IMG, "01-data.png"))
```

How do we make sense of all of these numbers and text? Visualising the data can 
give us a lot of insight into the underlying structure of the data (covered in a previous workshop).
The next step is to further *summarise* the data by means of a model. 

## What is a model?

> A **model** is a human construct/abstraction that summarises the data in a useful manner

Much like the abstract of a scientific paper encapsulates the main message of the paper,
a model captures the **salient** features/structure of the data. 

Models are built for various reasons:

* to enchance understanding of a complex phenomenon (e.g how does antibiotic resistance develop)
* to execute "what if" scenarios (e.g what happens if interest rates go up)
* to predict/forecast an outcome (e.g how many people will get influenza next winter)
* to control a system (e.g autonomous vehicles)

Models come in all kinds of "languages":

* a physical model e.g [medical training models](https://www.anatomystuff.co.uk/clinical-skills-training-models.html)
```{r, echo=FALSE, out.width="480px"}
knitr::include_graphics(file.path(IMG, "01-medical.png"))
```

* a verbal/pictorial model e.g [Francischetti *et al. Microcirculation* (2008)](https://onlinelibrary.wiley.com/doi/pdf/10.1080/10739680701451516)
```{r, echo=FALSE, out.width="480px"}
knitr::include_graphics(file.path(IMG, "01-biological.png"))
```

* a mathematical model
$$
\begin{aligned}
{\frac {\mathrm {d} x}{\mathrm {d} t}}& = \sigma (y-x),\\
{\frac {\mathrm {d} y}{\mathrm {d} t}}& = x(\rho -z)-y\\
{\frac {\mathrm {d} z}{\mathrm {d} t}}& = xy-\beta z
\end{aligned}
$$

**Assumptions** are an inherent part of every model; we cannot build models if we are not prepared
to make assumptions. The feasibility of these assumptions is context-dependent but also
on how the model will be used. 

Remember that models do **not** represent the real-world, they are merely an "abridged"
version of the real-world with many (many) caveats!!

> All models are wrong but some are useful - **George E.P. Box**

> For every complex question there is a simple and wrong solution - **A. Einstein**

## What is a statistical (stochastic) model?

> A **statistical model** is a mathematical model that makes a set of statistical assumptions in the form of probability distributions (i.e we assume that some variables follow a pre-defined probability distribution).

In this workshop we will solely focus on **linear** models. In a nutshell, this means
that we will be "simply" fitting straight lines (hyperplanes if we have multiple explanatory variables) 
through some data points.

## Illustrative example

Remember when you used to run simple lab experiments at school and plot data
onto graph paper? It used to look something like this (with different axes labels of course):

```{r, echo=FALSE, fig.height=6, fig.width=6}
set.seed(1201)
N <- 50
x <- runif(N, min=0, max=10)
y <- 2*x + 5 + rnorm(N, mean=0, sd=3)
plot(x, y, pch=19, xlab='Hours spent wathcing Game of Thrones (GoT) at the weekend',
     ylab='Minutes spent on monday talking about GoT at work', col='grey')
```

Once you plotted out all of the data points, you were asked to grab a ruler 
(a rigid, straight object) and find the line of best fit. To do this, you
used to try and get equal number of data points on each side of the line.

```{r, echo=FALSE, fig.height=6, fig.width=6}
fit <- lm(y ~ x)
plot(x, y, pch=19, xlab='Hours spent wathcing Game of Thrones (GoT) at the weekend',
     ylab='Minutes spent on monday talking about GoT at work', col='grey')
abline(lm(y~x), col='black', lwd=3)
```

**Congratulations!** Unknowingly, you were fitting a statistical model to your measured data! 
Your best fit in that case was equivalent to performing **ordinary least-squares**
which minimises the distance between the model (straight line) and your data points.
But more on that later.

The fitted linear model is of the form $y = mx + c$. Where $m$ is the gradient and 
$c$ is the intercept.

```{r, echo=FALSE, fig.height=6, fig.width=6}
plot(x, y, pch=19, xlab='Hours spent wathcing Game of Thrones (GoT) at the weekend',
     ylab='Minutes spent on monday talking about GoT at work', col='grey')
abline(lm(y~x), col='black', lwd=3)
arrows(0.1, 0.5, 0.1, coef(fit)[1], angle=20, lwd=3, col='red', code=3, length=0.15)
text(0.4, coef(fit)[1]/2, 'c', col='red', font=2, cex=1.2)
segments(7.5, predict(fit, data.frame(x=7.5)), 7.5, predict(fit, data.frame(x=7.5))-6, col='red', lwd=3)
segments(7.5, predict(fit, data.frame(x=7.5))-6, -7.5 + coef(fit)[2]*(6), 
         predict(fit, data.frame(x=7.5))-6, col='red', lwd=3)
text(7.4, 16, 'm = gradient', col='red', font=2, cex=1.2, pos=4)
```

At school you were inferring the parameters $m$ and $c$ by hand. In this workshop you will learn
how to do that in R. 

What are the statistical assumptions in this case? As we will see in the 
next chapter, in linear regression/modelling we assume that the **residuals** 
i.e the differences between the model (black line) and "reality" (observed data points), 
follow a normal/Gaussian distribution (a bell curve).

```{r, echo=FALSE}
d <- density(fit$residuals, kernel='gaussian')
hist(fit$residuals, xlab='residuals', main='Histogram of residuals', 
     freq=FALSE, xlim=c(-8, 8))
lines(d$x, d$y, lwd=3)
```

**Model checking** involves confirming that our statistical assumptions are sensible. 
We will delve into this in much detail later on, but just remember that when you hear people saying
"check that your data is normal", in the context of linear regression, what this means is that
the **residuals** are normal (which also means that the **response** variable is normal).

## Summary

* Models are useful abstractions of the data that can help us understand the underlying system
* Linear regression simply means fitting a "best" straight line (hyperplane) through the observed data points 
* Model checking is the process of verifying the statistical assumptions made