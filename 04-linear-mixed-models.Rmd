# Mixed effects models

## DATA

DATA!!!!

Firstly, install if necessary (using `install.packages()`; uncomment the code below if required) and load the `lme4` package [@lme4]:

```{r, message = F, warning = F}
## load libraries
library(tidyverse)
library(lme4)
library(car)
```

REML / ML model choice

F-test / chi-squared etc.

## Introduction

This practical will focus on how to analyse data when the experimental design (or the surveyed explanatory variables) obliges us to study non-independent experimental units. You will find yourself distinguishing between random effects and fixed effects. You will find yourself frustrated at the lack of consensus regarding how to simplify fixed factors in mixed models. It takes a long time to understand how to deal with blocks (and other random effects): don't expect understanding to come overnight, and do rely on books and websites to help you. But be warned, at this level of statistical prowess, much of the literature is written in Greek symbols and matrix algebra.

***Health Warning: some of these mixed-effects modelling packages are quickly evolving, and future updates might play some havoc with the coding, and outputs from, the scripts written here. You will need to keep your eye on the forums in the future if this happens. As always, let me or your demonstrators know if you encounter difficulties.***

## Randomised Complete Block Design

> Often you will find it impossible to allocate treatments to experimental units at random because they are structured in time or in space. This means that units are not independent of each other, hence their residuals will not be independent, and we have violated an important assumption of generalised linear modeling (and, indeed, of analysis of variance). However the recognition of natural structuring in our population of experimental units can be **VERY** useful. If the experimental design can be stratified so that all units that share a common 'trait' (e.g. share a corner of a field, or share a single incubator) can represent one or more full replicate of the proposed experiment, then we should use this natural structuring to absorb some of our enemy: noise (also called residual deviance).

An extreme case of using error-absorbers is the Randomised Complete Block Experimental Design. So-called because it includes blocks, each block contains a single replicate of the experiment, and experimental units **WITHIN** blocks are allocated to treatment combinations **AT RANDOM**. The concept is best explained using an example.

A microbiologist wishes to know which of four growth media is best for rearing large populations of anthrax, quickly. However, this poorly funded scientist does not own a large enough incubator in which to grow lots of replicate populations. Instead he requests space in five different incubators owned by other, better-funded researchers. Each incubator just has space for four bottles of medium. Our scientist allocates each growth medium to one bottle per incubator **AT RANDOM**, inoculates with anthrax then monitors population growth rate.

A schematic is given in Figure \@ref(fig:bacCabinets).

```{r, bacCabinets, fig.cap = "Schematic for bacterial growth example", echo = F, out.width = "80%", out.height = "80%"}
include_graphics("_img/bacCabinets.png")
```

The data are available in the 'bacCabinets.rds' file. Read in this dataset and familiarise yourself with its structure. 

```{r, echo = F}
bac <- readRDS("_data/bacCabinets.rds")
```

```{r, eval = F}
bac <- readRDS("bacCabinets.rds")
```

Let's do the analysis **WRONGLY** to begin with:

```{r bacwrong}
bac_lm <- lm(growth ~ media, data = bac)
drop1(bac_lm, test = "F")
```

```{task, title = "Question"}
Why is this wrong?
```

```{solution, title = "Answer"}
Because the analysis does not account for the cabinet effects. There are not 20 *independent* measurements here.
```

Let's refit the model with a `cabinet` effect, and then test for the statistical significance of `media`:

```{r}
bac_lm <- lm(growth ~ media + cabinet, data = bac)
anova(bac_lm, update(bac_lm, ~ . - media), test = "F")
```

This is better. We need a `cabinet` effect here to deal with the fact that measurements within cabinets are not independent.

> **Note**: here the `cabinet` effect **must** be included in the model. It does not make sense for us to drop the `cabinet` effect and test for whether it should be included or not. It needs to be included **by design**!

```{task}
Try with an **interaction** effect between `medai` and `cabinet`. What happens and why?
```

```{solution}

``{r}
bac_lm <- lm(growth ~ media * cabinet, data = bac)
summary(bac_lm)
``

We are faced with all these `NA`s because the model is **SATURATED**: there is no replication so we cannot 
calculate the residual deviance/variance. This also means that there are no degrees-of-freedom remaining in the model to calculate the residual sum-of-squares, and so we can't model simplify from this model.
```

Let's re-do this analysis using a **mixed model**. If you haven't already, you will need to load the `lme4` package:

```{r, eval = F}
library(lme4)
```
    
> **Fixed and random effects**: 
> 
> * `cabinet` is a **random** effect---we don't care about the identity of each cabinet, each cabinet is sampled from a population of possible cabinets, we just want to predict and absorb the variance in bacterial growth rate explained by cabinet;
> * `media` is a **fixed** effect---we chose the media to be tested, each media has a specific identity, we want to estimate the differences in bacterial growth between different media. 

Now on with the analysis:

```{r bac_lmer}
bac_lmer <- lmer(growth ~ media + (1 | cabinet), data = bac)
summary(bac_lmer)
```
    
> **Syntax for `lmer`**: The `lmer` command has two important sections. First is the 'fixed effects' part of the model (in this case, `growth ~ media`), which is identical to the model you would enter into `glm` if there were no problems of non-independence of data. The second part is where you enter the 'random effects' part of the model, where you tell R how the data is nested and where the non-independence lies. There is a whole world of possibilities here, from spatially autocorrelated variance/covariance matrices to independently varying random slopes and intercepts. We keep it simple in this course. For this example, we assume no interaction between media and cabinet, hence we just want to predict and absorb the additive variance due to different cabinets. Hence our random effect term is `(1 | cabinet)`. This means: 'the intercept of our linear model will vary according to cabinet'.

> **NOTE:** for various reasons the authors of `lme4` do not provide p-values as standard from a call to `summary()` or `anova()`. We can perform a **likelihood ratio test** for the impact of dropping `media` from the fitted model. The simplest way to do this is to use the `drop1()` function:
> 

```{r, warning = F, message = F}
drop1(bac_lmer)
```
    
> The 'intercept' of the `lmer` model is the mean growth rate in `media1` for an **AVERAGE** cabinet. `lmer` output also gives you information criteria about the model, tells you the standard deviation of the random effects, correlations between levels of fixed effects, and so on. Here we have used a **likelihood ratio test** to assess whether the removal of `media` corresponds to a statistically significantly inferior model fit, which in this case it does at both the 5% and 1% levels.
> 
> **Note**: we use a likelihood ratio test (LRT) here, rather than an F-test. The F-test would be OK in this particular case, with Gaussian error structure. LRTs are more general, and thus for consistency I will use these (or AIC) throughout where required.

## Non-independent data: pseudoreplication, nested variance and derived variable analysis

A motivating example: the value of endosymbionts to aphids.

Recently [Dave](http://biosciences.exeter.ac.uk/staff/index.php?web_id=david_hodgson) reviewed a paper submitted to Biology Letters, in which the fitness of 8 aphid clones was measured. Fitness was measured using 100 aphids per clone. Four of the clones harboured a single species of endosymbiont (bacteria that inhabit aphid cells and help them to synthesise essential amino acids), while four clones harboured multiple endosymbiont species. The hypothesis was that harbouring multiple species of endosymbiont is 'good' for fitness. The statistical analysis considered the fitness of each aphid as its response variable, and tested the influence of 'endosymbiont diversity' (one vs. many species) as a categorical explanatory variable. Importantly, the analysis correctly recognized that 100 aphids came from each clone, and therefore 'absorbed' the effect of aphid clone as a random effect in a mixed-effects model. 

However, the fundamental test of `fitness ~ endosymbiont.diversity` used **ALL THE DATA** and had 797 residual degrees of freedom (800 minus 2 (estimation of the two treatment means) minus 1 (estimation of the among-clone variance)). This is wrong. It's wrong because the diversity of endosymbionts is a feature of the aphid **CLONE**, not of the individual aphid. All the aphids within each clone shared lots of features alongside endosymbiont diversity. Absorbing the variation among clones is a good idea, but the analysis remains fundamentally **PSEUDOREPLICATED**. The only valid analysis to test this hypothesis is to calculate mean fitness per clone, and compare the means of the four clones with a single endosymbiont to the means of the four clones with more than one endosymbiont. The use of mixed models is not a panacea for poor experimental design.

Another way to think about this is to ask at what level of the experimental design is the explanatory variable manipulated? Here, endosymbiont diversity happens at the level of the **CLONE**. So, the test of the impact of endosymbiont diversity must be performed at the level of the **CLONE**. If the experimenter had manipulated endosymbiont diversity within each individual aphid, then the analysis could use all the aphids as experimental units, absorb the 'extra' similarities among clonemates by including `clone` as a random effect, and correctly use the test that they used. But they didn't do that so their paper got rejected. 

### Working with data

We take an example from Sokal & Rohlf (1981). The experiment involved a simple one-way anova with 3 treatments given to 6 rats. The analysis was complicated by the fact that three preparations were taken from the liver of each rat, and two readings of glycogen content were taken from each preparation. This generated 6 pseudoreplicates per rat to give a total of 36 readings in all. A schematic is given in Figure \@ref(fig:rats).

```{r, rats, fig.cap = "Schematic for glycogen in rats example", echo = F, out.width = "80%", out.height = "80%"}
include_graphics("_img/rats.png")
```

Clearly, it would be a mistake to analyse these data as if they were a straightforward one-way anova, because that would give us 33 degrees of freedom for error. In fact, since there are only two rats in each treatment, we have only one degree of freedom per treatment, giving a total of 3 d.f. for error. 
 
The variance is likely to be different at each level of this nested analysis because: 
 
1. the readings differ because of variation in the glycogen detection method within each liver sample (measurement error);
2. the pieces of liver may differ because of heterogeneity in the distribution of glycogen within the liver of a single rat;
3. the rats will differ from one another in their glycogen levels because of sex, age, size, genotype, etc.;
4. rats allocated different experimental treatments may differ as a result of the fixed effects of treatment. 

If all we want to test is whether the experimental treatments have affected the glycogen levels, then we are not interested in liver bits within rat's livers, or in preparations within liver bits. We could combine all the pseudoreplicates together, and analyse the 6 averages. This would have the virtue of showing what a tiny experiment this really was. This latter approach also ignores the nested sources of uncertainties. Instead we will use a linear mixed model.
 
The new concept here is that there are multiple random effects that are **nested**. First, read in the data:

```{r, echo = F}
rats <- readRDS("_data/rats.rds")
```

```{r, eval = F}
rats <- readRDS("rats.rds")
```

### The Wrong Analysis 

Here is what not to do (try it anyway)!

```{r wrong} 
rats_lm <- lm(Glycogen ~ Treatment * Rat * Liver, data = rats)
```

The model has been specified as if it were a full factorial with no nesting and no pseudoreplication. Note that the structure of the data allows this mistake to be made. It is a very common problem with data frames that include pseudoreplication. A summary of the model fit looks like this:

```{r}
summary(rats_lm)
drop1(rats_lm, test = "F")
```

This says that there was a highly statistically significant difference between the treatment means, at least some of the rats were statistically significantly different from one another, and there was a statistically significant interaction effect between treatments and rats. This is wrong! The analysis is flawed because it is based on the assumption that there is only one error variance and that its value is 21.2. This value is actually the measurement error; that is to say the variation between one reading and another from the same piece of liver. For testing whether the treatment has had any effect, it is the rats that are the replicates, and there were only 6 of them in the whole experiment. 
 
### Derived variables avoid pseudoreplication 

The idea is to get rid of the pseudoreplication by averaging over the liver bits and preparations for each rat. A useful way of averaging, in R, is to use `aggregate` (or `tidyverse`):

```{multCode}

``{r}
ratsNew <- aggregate(Glycogen ~ Rat + Treatment, data = rats, mean)
ratsNew
``

####

``{r}
ratsNew <- rats %>%
    group_by(Rat, Treatment) %>%
    summarise(Glycogen = mean(Glycogen))
ratsNew
``

```

Here, `ratsNew` is a new dataset (hopefully shorter than its ancestral dataset) but has the same column headings as `rats`. (Hence be careful to specify the **correct** data set using the `data` argument where necessary.)

```{r ratsagg}
ratsNew_lm <- lm(Glycogen ~ Treatment, data = ratsNew)
drop1(ratsNew_lm, test = "F")
```

This is a statistically valid analysis, but it ignores the uncertainties around the pseudo-replicates, and the interpretation of the response variable is actually the mean of a bunch of measurements, not the measurements themselves. So it's giving the correct result for a different model.


### Mixed model approach

#### Nested and crossed random effects

We have to be careful to get the model formula correct. Notice that in the `rats` data the `Rat` column is coded as a `0` or a `1`. This means that we could use the formula:

```{r, eval = F}
rats_lmer <- lmer(Glycogen ~ Treatment + (1 | Rat), data = rats)
```

**without** throwing an error.

```{task, title = "Question"}
Why is this wrong?
```

```{solution, title = "Answer"}
It is wrong because this would add **two** random effect terms, one for rat `0` and one for rat `1`. This is wrong. There are 6 rats altogether. The way that the data have been coded allows for these kinds of mistakes to happen. The same is true for `Liver`, which is coded as a `1`, `2` or `3`. This means that we could write:
    
``{r, eval = F}
rats_lmer <- lmer(Glycogen ~ Treatment + (1 | Rat) + (1 | Liver), data = rats)
``

thinking that we are including the correct random effects for `Rat` and `Liver`. In fact, this assumes that the data come from a **crossed** design, in which there are 2 rats and 3 parts of the liver, and that `Liver = 1` corresponds to the same type of measurement in rats `0` and `1` and so on. Sometimes this is appropriate, but not here!
    
The nature of the way that many data sets are coded makes these kinds of mistakes very easy to make!
```

In this case we must tell `lmer()` that `Liver` is **nested** within `Rat`, and `Rat` within `Treatment`. We can do this in various ways, but the easiest thing here is to generate a unique coding for each of the 6 rats.

```{multCode}

``{r, echo = F}
rats_orig <- rats
``

``{r}
rats$Rat <- paste(rats$Treatment, "_", rats$Rat)
rats$Rat <- as.numeric(factor(rats$Rat))
``

####

``{r, echo = F}
rats <- rats_orig
``

``{r}
rats <- rats %>%
    unite(Rat, Treatment, Rat, sep = "_", remove = F) %>%
    mutate(Rat = as.numeric(factor(Rat)))
``

```

We can use **nested** random effects to account for the hierarchy of measurements:

```{r}
rats_lmer <- lmer(Glycogen ~ Treatment + (1 | Rat / Liver), data = rats)
anova(rats_lmer, update(rats_lmer, ~ . - Treatment), test = "Chisq")
#drop1(rats_lmer, test = "Chisq")
```

> **Note**: the syntax `Rats / Liver` means that the `Liver` levels are **nested** within the `Rat` levels. In this case we do not have to recode `Liver` to be unique, since the nesting ensures that `lmer()` knows that `Liver = 1` inside `Rat = 1` is different from `Liver = 1` inside `Rat = 2`.

## Non-independent data: Split-plot analyses

Split-plot experiments are like nested designs in that they involve plots of different sizes and hence have multiple error terms (one error term for each plot size). They are also like nested designs in that they involve pseudoreplication: measurements made on the smaller plots are pseudoreplicates as far as the treatments applied to larger plots are concerned. This is spatial pseudoreplication, and arises because the smaller plots nested within the larger plots are not spatially independent of one another. The only real difference between nested analysis and split plot analysis is that other than blocks, all of the factors in a split-plot experiment are typically fixed effects, whereas in most nested analyses most (or all) of the factors are random effects.  
 
The only things to remember about split plot experiments are that:
 
1. We need to draw up as many anova tables as there are plot sizes, 
2. the error term in each table is the interaction between blocks and all factors applied at that plot size or larger. This is an extension of the logic for Randomized Complete Block Designs, in which fixed effects are tested against interactions with block.
 
This experiment involves the yield of cereals in a factorial experiment with 3 treatments, each applied to plots of different sizes within 4 blocks. The largest plots (half of each block) were irrigated or not because of the practical difficulties of watering large numbers of small plots. Next, the irrigated plots were split into 3 smaller split-plots and seeds were sown at different densities. Again, because the seeds were machine sown, larger plots were preferred. Finally, each sowing density plot was split into 3 small split-split plots and fertilisers applied by hand (N alone, P alone and N + P together). The data look like:

```{r splityield0, echo = F}
splityield <- readRDS("_data/splityield.rds")
temp <- tapply(splityield$yield, rev(list(Block = splityield$block, Irrigation = splityield$irrigation, Fertiliser = splityield$fertilizer, density = splityield$density)), function(x) x)
temp.irr <- temp[, , "irrigated", ]
temp.cont <- temp[, , "control", ]
    
temp.irr <- do.call("rbind", lapply(1:4, function(i, x) x[, , i], x = temp.irr))
temp.cont <- do.call("rbind", lapply(1:4, function(i, x) x[, , i], x = temp.cont))
temp <- cbind(temp.cont, temp.irr)
temp <- cbind(rownames(temp), temp)
temp <- rbind(colnames(temp), temp)
colnames(temp) <- c("", "", "Control", "", "", "Irrigated", "")
rownames(temp) <- c("", "", "Block A", "","", "Block B", "","", "Block C", "","", "Block D", "")
temp[1, 1] <- "**Density / Fertiliser**"
library(pander)
panderOptions("table.split.table", Inf)
pander(temp, style = "rmarkdown", table.emphasize.rownames = F)
```

First, let's read in the data and take a look at it.

```{r, eval = F}
splityield <- readRDS("splityield.rds")
head(splityield)
summary(splityield)
```

```{r, echo = F}
splityield <- readRDS("_data/splityield.rds")
head(splityield)
summary(splityield)
```

### Analysing split-plot designs using `lmer`

Now, how do we set up a mixed effects model to analyse these data? `block` is the only random effect but our data are nested. Our fixed effects are `irrigation`, `density` and `fertilizer`. Here is the model, including prediction of the variance due to block's random effect on the intercept of the model.

**Note**: Ordinarily we would write the nested random effect terms using the syntax:

```{r, error = TRUE, tidy = F}
split_lmer <- lmer(yield ~ irrigation * density * fertilizer + 
                       (1 | block / irrigation / density / fertilizer), data = splityield)
```

However, if you run this, you will notice that the function returns an error. This is because there are no replicates within the final nesting (i.e. the table above on has a single replicate in each cell). To overcome this we need to remove the final nesting. Unfortunately, this leads to a rather more horrible formula. 

Another way of writing the command above is:

```{r, eval = F, tidy = F}
split_lmer <- lmer(yield ~ irrigation * density * fertilizer + (1 | block) + 
                       (1 | block:irrigation) + (1 | block:irrigation:density) + 
                       (1 | block:irrigation:density:fertilizer), data = splityield)
```

Pretty grim eh? However, in this case we note that the final nested random effect only has one replicate per group, so we simply remove this term. Leaving:

```{r, tidy = F}
split_lmer <- lmer(yield ~ irrigation * density * fertilizer + (1 | block) + 
       (1 | block:irrigation) + (1 | block:irrigation:density), 
       data = splityield)
drop1(update(split_lmer, REML = F), test = "Chisq")
```
We can see that the three-way interaction term is not statistically significant, so we can drop it from the model and then test dropping the two-way interaction effects.

```{r, tidy = F}
split_lmer <- update(split_lmer, ~ . - irrigation:density:fertilizer)
drop1(update(split_lmer, REML = F), test = "Chisq")
```

We are thus safe to remove the `density:fertilizer` interaction term:

```{r, tidy = F}
split_lmer <- update(split_lmer, ~ . - density:fertilizer)
drop1(update(split_lmer, REML = F), test = "Chisq")
```

We thus have a simplified final model. If you really want to know all the coefficients, type `summary(split.lmer)`, but here we will try to understand the interaction terms graphically.

A useful way to understand these is to use the `interaction.plot()` function. The variables are listed in a non-obvious order: first the factor to go on the $x$-axis, then the factor to go as different lines on the plot, then the response variable. There are 3 plots to look at so we make a $2 \times 2$ plotting area: 
 
```{r, fig.width = 10, fig.height = 10, fig.show = "hold"}
par(mfrow = c(2, 2))
interaction.plot(splityield$fertilizer, splityield$density, splityield$yield) 
interaction.plot(splityield$fertilizer, splityield$irrigation, splityield$yield) 
interaction.plot(splityield$density, splityield$irrigation, splityield$yield)
par(mfrow = c(1, 1))
```

The really pronounced interaction is that between irrigation and density, with a reversal of the high to low density difference on the irrigated and control plots. Interestingly, this is not the most statistically significant interaction. That honour goes to the `fertilizer:irrigation` interaction (top right graph). 

Overall, the 3-way interaction was not statistically significant, nor was the 2-way interaction between density and fertilizer. The 2-way interaction between irrigation and fertilizer, however, was highly statistically significant and, not surprisingly, there was a highly statistically significant main effect of fertilizer. 

## Absorbing the influence of random effects

Eight groups of students walked through Hell's Gate National Park and estimated the distance to groups or individuals of several species of mammalian herbivores. Here we work with data on distances to three key species: Thomson's gazelle, warthog and zebra. We are interested in whether distances to these animals differ among species, whether distances depend on animal group size, and also whether the relationship between distance and group size differs among species. Previously we have worked with data from the first and the last group to walk through Hell's Gate. Here we will work with data from all the groups. The data is in file "hg.rds".

A na&iuml;ve analysis would ignore the identity of the student group, and therefore treat all the distance observations as independent. It would probably fit a `lm` of distance against species, number and the interaction between species and number. My own exploration of the data suggests that raw distances are skewed, and that a log-transformation improves homoscedasticity and normality of residuals. So before analysing we do this transformation.

```{r, echo = F}
hg <- readRDS("_data/hg.rds")
```

```{r, eval = F}
hg <- readRDS("hg.rds")
```

```{r}
hg$ldist <- log(hg$Distance)
hg_lm <- lm(ldist ~ Species * Number, data = hg)
drop1(hg_lm, test = "F")
```

This suggests a statistically significant interaction between `Species` and `Number` ($F_{2, 456} = 5.78$, $p = 0.003$).

```{r sumhg}
summary(hg_lm)
```

The interaction appears to be driven by a decrease in distance with increasing group size, but only in zebras. We can draw the relevant figure (notice that this is where `tidyverse` comes into its own, with much more concise code resulting in a better plot).

```{multCode}

Generate 'fake' data in order to plot fitted lines:

``{r}
newdata <- expand.grid(
    Number = seq(min(hg$Number), max(hg$Number), length = 100),
    Species = levels(hg$Species)
)
``

Now get the predicted values for the fake data, and back-transform log-distance to real distances, using the `exp` command:

``{r}
newdata$Distance <- exp(predict(hg_lm, newdata))
``

Now plot the fitted lines against the observed data points:

``{r}
plot(Distance ~ Number, data = hg, type = "n")
points(Distance ~ Number, 
       data = hg, 
       subset = (Species == "thomsons gazelle"), 
       pch = 16)
points(Distance ~ Number, 
       data = hg, 
       subset = (Species == "warthog"), 
       pch = 16,
       col = "red")
points(Distance ~ Number, 
       data = hg, 
       subset = (Species == "zebra"), 
       pch = 16,
       col = "blue")
lines(Distance ~ Number, 
      data = newdata, 
      subset = (newdata$Species == "thomsons gazelle"), 
      lwd = 2)
lines(Distance ~ Number, 
      data = newdata,
      subset = (newdata$Species == "warthog"), 
      lwd = 2, 
      col = "red")
lines(Distance ~ Number, 
      data = newdata,
      subset = (newdata$Species == "zebra"), 
      lwd = 2, 
      col = "blue")
legend(15, 1000, 
       pch = rep(16, 3), 
       col = c("black", "red", "blue"), 
       legend = c("gazelle", "warthog", "zebra"))
``

####

Generate 'fake' data in order to plot fitted lines:

``{r}
newdata <- expand.grid(
        Number = seq(min(hg$Number), max(hg$Number), length = 100),
        Species = levels(hg$Species)
    )
``

Now get the predicted values for the fake data, and back-transform log-distance to real distances, using the `exp` command:

``{r}
newdata <- newdata %>%
    mutate(Distance = exp(predict(hg_lm, newdata)))
``

Now plot the fitted lines against the observed data points:

``{r}
ggplot(hg, aes(x = Number, y = Distance, col = Species)) +
    geom_point() + 
    geom_line(data = newdata)
``

```

This na&iuml;ve analysis is fundamentally flawed because the distance estimations are not independent of each other. Several estimates come from each of 8 student groups, and it's easy to imagine (or remember) that observers vary dramatically in the bias of their distance estimates (and indeed in their estimates of group size, possibly even species identity). It's entirely possible that a group of observers who estimate "short" distances also encountered strangely large groups of zebras: such a correlation between `Group.Name` and `Distance` could drive the observed relationship for zebras. 

One solution would be to fit `Group.Name` as a categorical explanatory variable to our model. This approach has two main problems:

1. It wastes valuable degrees of freedom (8 student groups, therefore 7 d.f. as a main effect and 7 d.f. more for each interaction), giving the residual variance fewer d.f. and therefore weakening the power of our analysis. 
2. It could easily result in a complicated minimal adequate model involving something that we don't really care about and which does not appear in our hypothesis: `Group.Name`.

Instead we would like to be able to absorb the variance in distance estimates among student groups. Such a model would estimate the relationship between `Distance`, `Species` and `Number` for an **average** group of observers. It could also tell us just how much variance exists among groups of observers.

This is a classic example of where a **mixed-effects model** is useful. We care about the influence of `Species` and `Number` on `Distance`. These are **fixed effects**. We know that `Group.Name` will have an influence on `Distance`, but the categories of `Group.Name` are uninformative to the lay reader and cannot be replicated in future field trips. Hence we treat `Group.Name` as a **random effect**. An ideal mixed-effects model will test the influence of `Species` and `Number` on `Distance`, meanwhile absorbing the influence of `Group.Name`.

```{r}
hg_lmer <- lmer(ldist ~ Species * Number + (1 | Group.Name), data = hg)
```

Note that there is no nesting going on here; `Species` and `Number` have the same interpretation regardless of the `Group.Name`. Let's take a look at the summary of the fitted model.

```{r}
summary(hg_lmer)
```

This summary table is packed with information. First, we can see that the standard deviation in `Distance` among student groups is 0.46, which is almost as big as the residual standard deviation of 0.55. This suggests variation among observers is important and could dramatically change our results. Second, the estimates of slopes and differences between species are going in the 'same direction' as the estimates from the na&iuml;ve `lm()` analysis, but seem to be smaller. 

With **unbalanced** experimental designs like this, the fixed effects part of the model cannot be simplified in the usual way. This is because the removal of a fixed effect will **fundamentally** change the model fit: it's not just a case of dumping the deviance explained by a fixed effect into the residual deviance, because the residual deviance is different for each nested level of the experimental design. Furthermore, if the experiment is unbalanced, we should beware of using F-tests to check the statistical significance of terms. Instead, we should use AIC or **likelihood ratio tests**, and if we do this we need to change the fitting mechanism from REML to ML whilst performing LRT. (We do this *in situ* using the `update()` function.)

```{r}
drop1(update(hg_lmer, REML = F), test = "Chisq")
```

As usual, likelihood ratio tests in this context have a chi-squared distribution with d.f. equal to the difference in d.f. between the models. Here we have 8 - 6 = 2 d.f. So, here we find no evidence for an interaction between `Species` and `Number` ($X^2_2 = 2.16$, $p = 0.34$).

Since we have an **unbalanced** design here, we can continue model simplification by dropping each of the main effect terms from the current model and seeing which has the largest impact on model fit (in this case the model simplification will be different depending on the ***order*** in which variables are added/removed from the model). 

```{r}
## set baseline model from previous round
hg_lmer <- update(hg_lmer, ~ . - Species:Number)

## now drop terms compare to baseline model
drop1(update(hg_lmer, REML = F), test = "Chisq")
```

Here dropping `Number` has little impact on the model fit, whereas dropping `Species` results in a statistically significantly inferior fit. Hence the next stage is to drop `Number`. Therefore our baseline model now has just the main effect for `Species`. The final stage is to drop `Species` and assess the change in fit relative to our current model.

```{task, title = "Question"} 
Why do we need to do this last step, since we assessed the impact of removing `Species` before?
```

```{solution, title = "Answer"}
Because the LRT at the previous stage was comparing the baseline model `ldist ~ Species + Number` with the model `ldist ~ Number`. Since we have now removed `Number`, the new LRT will compare the baseline model `ldist ~ Species` with the null model `ldist ~ 1` (i.e. a model with just a single intercept value). In **unbalanced** designs these two comparisons are different tests and thus can give different results.
```

```{r}
## set baseline model from previous round
hg_lmer <- update(hg_lmer, ~ . - Number)

## now drop terms compare to baseline model
drop1(update(hg_lmer, REML = F), test = "Chisq")
```

This suggests there is a clear effect of `Species` identity on `log(Distance)` ($X^2_2 = 54.0$, $p < 0.001$).

Now we'd like to see our model summary (notice that R has automatically kept the REML fit; since we only converted the model objects to use ML during the model comparison exercise):

```{r}
summary(hg_lmer)
```

This final summary tells us that the standard deviation among observer groups is almost as big as the residual standard deviation in distances. It tells us that both warthogs and zebra are closer to the observer than Thomson's gazelles, which are approximately $e^{5.44} = 230$ metres away on average. The number of animals in a group has negligible influence on distance given the uncertainties in the data.

We're now left with a final model that has a single categorical variable. We could provide a boxplot of the raw data to describe it, but in this instance it would be misleading because it would not differentiate between observer-level noise and residual noise. In this instance we will produce confidence intervals for the mean distance from the road for each species and plot them.
